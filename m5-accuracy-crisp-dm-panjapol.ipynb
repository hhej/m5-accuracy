{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Walmart Sales Forecasting Project","metadata":{}},{"cell_type":"markdown","source":"## Sales Forecasting","metadata":{}},{"cell_type":"markdown","source":"Sales Forecasting is a very important area in field of business management. It helps the store retailers to maintain there stocks according to demand they are expecting, thus maximizing their profit and minimise burden of management of product stocks. It is similar to weather forecasting as both types of forecasting rely on science and historical data.\nWhile a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses.\nForecasting can drive sales by processing just-in-time orders efficiently.","metadata":{}},{"cell_type":"markdown","source":"## Problem Statement","metadata":{}},{"cell_type":"markdown","source":"We have Sales data for Walmart Stores in 3 states(California, Texas, Wisconsin) for 3 categoies of data (HOBBIES, FOOD, HOSEHOLD) from year 2011 to 2016. We want to use this data to predict sales for next 28 days using ML techniques.","metadata":{}},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"markdown","source":"\nWe have taken dataset from kaggle Competition named \"M5 Forecasting - Accuracy Estimate the unit sales of Walmart retail goods\"\n\nWe have been given sales of product for 1913 days amd we need to predict sales of next 28 days for each product.\n\nWe have been given several Dataframes Like:-\n\n\n* calendar.csv - Contains information about the dates on which the products are sold.\n* sell_prices.csv - Contains information about the price of the products sold per store and date.\n* sales_train_validation.csv - Contains the historical daily unit sales data per product and store d_1 - d_1913.\n* sales_train_evaluation.csv - Includes sales d_1 - d_1941.","metadata":{}},{"cell_type":"markdown","source":"## Metric Used","metadata":{}},{"cell_type":"markdown","source":"* This competition uses a Weighted Root Mean Squared Scaled Error (RMSSE).\n* We have used WRMSSE instead of Simple RMSE(Root Mean Squared Error) because in this dataset there are lot of zero sales so even if our model predicts most sales near to zero our model will give good performance, WRMSSE takes this in account.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom lightgbm import Dataset,train,plot_importance\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\nfrom sklearn.model_selection import KFold, train_test_split, GridSearchCV, cross_val_score, TimeSeriesSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LassoCV, RidgeCV\nimport gc\nimport os\nimport random\nfrom itertools import cycle\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-21T09:39:28.598018Z","iopub.execute_input":"2021-10-21T09:39:28.599013Z","iopub.status.idle":"2021-10-21T09:39:31.035941Z","shell.execute_reply.started":"2021-10-21T09:39:28.598888Z","shell.execute_reply":"2021-10-21T09:39:31.034857Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"random.seed(42)\nnp.random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:31.037820Z","iopub.execute_input":"2021-10-21T09:39:31.038335Z","iopub.status.idle":"2021-10-21T09:39:31.043990Z","shell.execute_reply.started":"2021-10-21T09:39:31.038291Z","shell.execute_reply":"2021-10-21T09:39:31.043066Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    \n    '''\n    reduce the memory usage of the dataframe by downcasting \n    the int and float to avoid the memory error\n    '''\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:31.046334Z","iopub.execute_input":"2021-10-21T09:39:31.046807Z","iopub.status.idle":"2021-10-21T09:39:31.070996Z","shell.execute_reply.started":"2021-10-21T09:39:31.046737Z","shell.execute_reply":"2021-10-21T09:39:31.068972Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def read_data(evaluation):\n    \n    '''\n    read data and reduce memory\n    '''\n    \n    print('Reading files...')\n    \n    calendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\n    calendar = reduce_mem_usage(calendar)\n    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n    \n    sell_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\n    sell_prices = reduce_mem_usage(sell_prices)\n    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n    \n    if evaluation:\n        sales_train_validation = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n        print('Sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n    else:\n        sales_train_validation = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv')\n        print('Sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n          \n    submission = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv')\n    submission = reduce_mem_usage(submission)\n    \n    return calendar, sell_prices, sales_train_validation, submission","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:31.073292Z","iopub.execute_input":"2021-10-21T09:39:31.074003Z","iopub.status.idle":"2021-10-21T09:39:31.092037Z","shell.execute_reply.started":"2021-10-21T09:39:31.073952Z","shell.execute_reply":"2021-10-21T09:39:31.090938Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def melt_and_merge_for_eda(calendar, sell_prices, sales_train_validation, submission, nrows = 30000000):\n    \n    sales_train_validation = pd.melt(sales_train_validation,\n                                     id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                                     var_name = 'day',\n                                     value_name = 'demand')\n    sales_train_validation = reduce_mem_usage(sales_train_validation)\n    \n    test2_rows = [row for row in submission['id'] if 'evaluation' in row]\n    test2 = submission[submission['id'].isin(test2_rows)]\n\n    test2.columns = ['id', 'd_1942', 'd_1943', 'd_1944', 'd_1945', 'd_1946', 'd_1947', 'd_1948', 'd_1949', 'd_1950', 'd_1951', 'd_1952', 'd_1953', 'd_1954', 'd_1955', 'd_1956', 'd_1957', 'd_1958', 'd_1959', \n                      'd_1960', 'd_1961', 'd_1962', 'd_1963', 'd_1964', 'd_1965', 'd_1966', 'd_1967', 'd_1968', 'd_1969']\n\n    product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n\n    test2 = test2.merge(product, how = 'left', on = 'id')\n    test2 = pd.melt(test2, \n                    id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                    var_name = 'day', \n                    value_name = 'demand')\n\n    sales_train_validation['part'] = 'train'\n    test2['part'] = 'test'\n    \n    data = pd.concat([sales_train_validation,test2], axis = 0)\n    del sales_train_validation,test2\n    gc.collect()\n    \n    data.reset_index(drop=True,inplace=True)\n    data = data.loc[30000000:]\n    #calendar.drop(['weekday', 'wday', 'month', 'year'], inplace = True, axis = 1)\n    data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n    data.drop(['d', 'day'], inplace = True, axis = 1)\n    del calendar,product\n    gc.collect()\n    \n    data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n    del sell_prices\n    gc.collect()\n    \n    print('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:31.093830Z","iopub.execute_input":"2021-10-21T09:39:31.094388Z","iopub.status.idle":"2021-10-21T09:39:31.114232Z","shell.execute_reply.started":"2021-10-21T09:39:31.094318Z","shell.execute_reply":"2021-10-21T09:39:31.113322Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def melt_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows = 30000000):\n    \n    sales_train_validation = pd.melt(sales_train_validation,\n                                     id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                                     var_name = 'day',\n                                     value_name = 'demand')\n    sales_train_validation = reduce_mem_usage(sales_train_validation)\n    \n    test2_rows = [row for row in submission['id'] if 'evaluation' in row]\n    test2 = submission[submission['id'].isin(test2_rows)]\n\n    test2.columns = ['id', 'd_1942', 'd_1943', 'd_1944', 'd_1945', 'd_1946', 'd_1947', 'd_1948', 'd_1949', 'd_1950', 'd_1951', 'd_1952', 'd_1953', 'd_1954', 'd_1955', 'd_1956', 'd_1957', 'd_1958', 'd_1959', \n                      'd_1960', 'd_1961', 'd_1962', 'd_1963', 'd_1964', 'd_1965', 'd_1966', 'd_1967', 'd_1968', 'd_1969']\n\n    product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n\n    test2 = test2.merge(product, how = 'left', on = 'id')\n    test2 = pd.melt(test2, \n                    id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                    var_name = 'day', \n                    value_name = 'demand')\n\n    sales_train_validation['part'] = 'train'\n    test2['part'] = 'test'\n    \n    data = pd.concat([sales_train_validation,test2], axis = 0)\n    del sales_train_validation,test2\n    gc.collect()\n    \n    data.reset_index(drop=True,inplace=True)\n    data = data.loc[30000000:]\n    calendar.drop(['weekday', 'wday', 'month', 'year'], inplace = True, axis = 1)\n    data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n    data.drop(['d', 'day'], inplace = True, axis = 1)\n    \n    data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n    del calendar,sell_prices,product\n    gc.collect()\n    \n    print('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:31.115887Z","iopub.execute_input":"2021-10-21T09:39:31.116454Z","iopub.status.idle":"2021-10-21T09:39:31.137314Z","shell.execute_reply.started":"2021-10-21T09:39:31.116400Z","shell.execute_reply":"2021-10-21T09:39:31.136263Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def transform(data):\n    \n    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in nan_features:\n        data[feature].fillna('unknown', inplace = True)\n                \n    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in cat:\n        encoder = preprocessing.LabelEncoder()\n        data[feature] = encoder.fit_transform(data[feature])\n        \n    print('fillna and encoded')\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:31.140384Z","iopub.execute_input":"2021-10-21T09:39:31.141488Z","iopub.status.idle":"2021-10-21T09:39:31.162492Z","shell.execute_reply.started":"2021-10-21T09:39:31.141428Z","shell.execute_reply":"2021-10-21T09:39:31.156986Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def features_engineering(data):\n    \n    data['lag_t28'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    data['lag_t29'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(29))\n    data['lag_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(30))\n    data['rolling_mean_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n    data['rolling_std_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    data['rolling_mean_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n    data = reduce_mem_usage(data)\n    \n    data['rolling_std_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n    data['rolling_skew_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).skew())\n    data['rolling_kurt_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).kurt())\n    data['rolling_mean_t90'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n    data['rolling_mean_t180'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n\n    data['lag_price_t1'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n    data['price_change_t1'] = (data['lag_price_t1'] - data['sell_price']) / (data['lag_price_t1'])\n    data['rolling_price_max_t365'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n    data['price_change_t365'] = (data['rolling_price_max_t365'] - data['sell_price']) / (data['rolling_price_max_t365'])\n    data['rolling_price_std_t7'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n    data['rolling_price_std_t30'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n    data.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)\n    \n    data['date'] = pd.to_datetime(data['date'])\n    data['year'] = data['date'].dt.year\n    data['month'] = data['date'].dt.month\n    data['week'] = data['date'].dt.week\n    data['day'] = data['date'].dt.day\n    data['dayofweek'] = data['date'].dt.dayofweek\n    data['isweekend'] = data['dayofweek'].apply(lambda x: 1 if x==5 or x==6 else 0)\n    \n    data['revenue'] = data['demand'] * data['sell_price']\n    data['lag_revenue_t1'] = data.groupby(['id'])['revenue'].transform(lambda x: x.shift(28))\n    data['rolling_revenue_std_t28'] = data.groupby(['id'])['lag_revenue_t1'].transform(lambda x: x.rolling(28).std())\n    data['rolling_revenue_mean_t28'] = data.groupby(['id'])['lag_revenue_t1'].transform(lambda x: x.rolling(28).mean())\n    data.drop(['revenue'],axis=1,inplace=True)\n    data = reduce_mem_usage(data)\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:31.167665Z","iopub.execute_input":"2021-10-21T09:39:31.168239Z","iopub.status.idle":"2021-10-21T09:39:31.197949Z","shell.execute_reply.started":"2021-10-21T09:39:31.168192Z","shell.execute_reply":"2021-10-21T09:39:31.196871Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def split_data(data):\n    x_train = data[data['part'] == 'train']\n    y_train = x_train['demand']\n    x_val = data[(data['date'] > '2016-04-24') & (data['part'] == 'train')]\n    y_val = x_val['demand']\n    test = data[data['date'] > '2016-04-24']\n    test.loc[test['part']=='train','id'] = test.loc[test['part']=='train','id'].str.replace('_evaluation','_validation')\n    x_train.drop(['demand','part',],inplace=True,axis=1)\n    x_val.drop(['demand','part',],inplace=True,axis=1)\n    test.drop(['demand','part',],inplace=True,axis=1)\n    \n    del data\n    gc.collect()\n    \n    return x_train, y_train, x_val, y_val, test","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:31.199487Z","iopub.execute_input":"2021-10-21T09:39:31.200063Z","iopub.status.idle":"2021-10-21T09:39:31.217398Z","shell.execute_reply.started":"2021-10-21T09:39:31.200014Z","shell.execute_reply":"2021-10-21T09:39:31.216329Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def dataset(category,category2):\n    \n    tindex = x_train[(x_train['dept_id']==category) & (x_train['store_id']==category2)].index.values\n    vindex = x_val[(x_val['dept_id']==category)&(x_val['store_id']==category2)].index.values\n    \n    x_t,x_v,y_t,y_v,t =  x_train[(x_train['dept_id']==category) & (x_train['store_id']==category2)],x_val[(x_val['dept_id']==category)&(x_val['store_id']==category2)],y_train.loc[tindex],y_val.loc[vindex],test[(test['dept_id']==category) &(test['store_id']==category2)]\n    \n    x_train.drop(tindex,axis=0,inplace=True)\n    x_val.drop(vindex,axis=0,inplace=True)\n    test.drop(test[(test['dept_id']==category) &(test['store_id']==category2)].index.values,axis=0,inplace=True)\n    \n    return x_t,x_v,y_t,y_v,t","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:31.221233Z","iopub.execute_input":"2021-10-21T09:39:31.222046Z","iopub.status.idle":"2021-10-21T09:39:31.237072Z","shell.execute_reply.started":"2021-10-21T09:39:31.221993Z","shell.execute_reply":"2021-10-21T09:39:31.236148Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"calendar, sell_prices, sales_train_validation, submission = read_data(True)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:31.238493Z","iopub.execute_input":"2021-10-21T09:39:31.238986Z","iopub.status.idle":"2021-10-21T09:39:43.539897Z","shell.execute_reply.started":"2021-10-21T09:39:31.238938Z","shell.execute_reply":"2021-10-21T09:39:43.538035Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of calender.csv\", calendar.shape)\nprint(\"Shape of sell_price.csv\", sell_prices.shape)\nprint(\"Shape of sales_train_validation.csv\", sales_train_validation.shape)\nprint(\"Shape of submission.csv\",submission.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:43.542138Z","iopub.execute_input":"2021-10-21T09:39:43.542498Z","iopub.status.idle":"2021-10-21T09:39:43.549860Z","shell.execute_reply.started":"2021-10-21T09:39:43.542453Z","shell.execute_reply":"2021-10-21T09:39:43.548446Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"calendar.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:43.551522Z","iopub.execute_input":"2021-10-21T09:39:43.552271Z","iopub.status.idle":"2021-10-21T09:39:43.581625Z","shell.execute_reply.started":"2021-10-21T09:39:43.552199Z","shell.execute_reply":"2021-10-21T09:39:43.580633Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"sell_prices.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:43.582953Z","iopub.execute_input":"2021-10-21T09:39:43.583624Z","iopub.status.idle":"2021-10-21T09:39:43.595238Z","shell.execute_reply.started":"2021-10-21T09:39:43.583585Z","shell.execute_reply":"2021-10-21T09:39:43.594603Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"sales_train_validation.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:43.596303Z","iopub.execute_input":"2021-10-21T09:39:43.597080Z","iopub.status.idle":"2021-10-21T09:39:43.827477Z","shell.execute_reply.started":"2021-10-21T09:39:43.597045Z","shell.execute_reply":"2021-10-21T09:39:43.826498Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:43.829986Z","iopub.execute_input":"2021-10-21T09:39:43.830238Z","iopub.status.idle":"2021-10-21T09:39:43.849974Z","shell.execute_reply.started":"2021-10-21T09:39:43.830207Z","shell.execute_reply":"2021-10-21T09:39:43.849375Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"data = melt_and_merge_for_eda(calendar, sell_prices, sales_train_validation,\n                      submission, nrows = 30000000)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:39:43.851033Z","iopub.execute_input":"2021-10-21T09:39:43.851398Z","iopub.status.idle":"2021-10-21T09:42:37.332315Z","shell.execute_reply.started":"2021-10-21T09:39:43.851342Z","shell.execute_reply":"2021-10-21T09:42:37.331170Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\nfor feature in nan_features:\n    data[feature].fillna('unknown', inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:42:37.333730Z","iopub.execute_input":"2021-10-21T09:42:37.334005Z","iopub.status.idle":"2021-10-21T09:42:44.845312Z","shell.execute_reply.started":"2021-10-21T09:42:37.333966Z","shell.execute_reply":"2021-10-21T09:42:44.844439Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:42:44.846826Z","iopub.execute_input":"2021-10-21T09:42:44.847436Z","iopub.status.idle":"2021-10-21T09:42:44.872969Z","shell.execute_reply.started":"2021-10-21T09:42:44.847390Z","shell.execute_reply":"2021-10-21T09:42:44.872140Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"data.columns.values","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:42:44.874647Z","iopub.execute_input":"2021-10-21T09:42:44.875306Z","iopub.status.idle":"2021-10-21T09:42:44.882792Z","shell.execute_reply.started":"2021-10-21T09:42:44.875260Z","shell.execute_reply":"2021-10-21T09:42:44.881667Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:42:44.884061Z","iopub.execute_input":"2021-10-21T09:42:44.884397Z","iopub.status.idle":"2021-10-21T09:42:44.912156Z","shell.execute_reply.started":"2021-10-21T09:42:44.884330Z","shell.execute_reply":"2021-10-21T09:42:44.911381Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"data['store_id'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:42:44.914080Z","iopub.execute_input":"2021-10-21T09:42:44.914571Z","iopub.status.idle":"2021-10-21T09:42:47.325187Z","shell.execute_reply.started":"2021-10-21T09:42:44.914520Z","shell.execute_reply":"2021-10-21T09:42:47.324292Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"data['state_id'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:42:47.326474Z","iopub.execute_input":"2021-10-21T09:42:47.326692Z","iopub.status.idle":"2021-10-21T09:42:49.663718Z","shell.execute_reply.started":"2021-10-21T09:42:47.326665Z","shell.execute_reply":"2021-10-21T09:42:49.662790Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"(data['date'].min(), data['date'].max())","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:42:49.664853Z","iopub.execute_input":"2021-10-21T09:42:49.665074Z","iopub.status.idle":"2021-10-21T09:42:58.518872Z","shell.execute_reply.started":"2021-10-21T09:42:49.665048Z","shell.execute_reply":"2021-10-21T09:42:58.517974Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"sales_train_validation = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv')\nsales_train_validation = reduce_mem_usage(sales_train_validation)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:42:58.520386Z","iopub.execute_input":"2021-10-21T09:42:58.520619Z","iopub.status.idle":"2021-10-21T09:45:45.738248Z","shell.execute_reply.started":"2021-10-21T09:42:58.520583Z","shell.execute_reply":"2021-10-21T09:45:45.737191Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"df=data.groupby('cat_id').mean()\ndf.reset_index(level=0,inplace=True)\nplt.figure(figsize=(20,10))\nplt.subplot(121)\nsns.barplot(x='cat_id',y='demand',data=df)\nplt.title(\"Bar-Graph for Avg Sales According to each categories\")\n\nplt.subplot(122)\ndf=data.groupby('cat_id').sum()\ndf.reset_index(level=0,inplace=True)\ndf['perc']=df['demand']/sum(df['demand'].values)*100\nplt.pie(df['perc'].values,labels=df['cat_id'].values,shadow=True,autopct='%1.1f%%')\nplt.title(\"Pie Chart showing total sales for each categories\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:45:45.740083Z","iopub.execute_input":"2021-10-21T09:45:45.740433Z","iopub.status.idle":"2021-10-21T09:46:18.363683Z","shell.execute_reply.started":"2021-10-21T09:45:45.740382Z","shell.execute_reply":"2021-10-21T09:46:18.362492Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Total sales for FOODS is the most.","metadata":{}},{"cell_type":"code","source":"sales_train_validation['total_sales'] = sales_train_validation.sum(axis=1)\nsns.catplot(x='cat_id', y='total_sales',\n           hue='state_id',\n           data=sales_train_validation, kind='bar',\n           height=8, aspect=1)\nplt.title('Total Sales on Categories by State')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:46:18.365133Z","iopub.execute_input":"2021-10-21T09:46:18.365936Z","iopub.status.idle":"2021-10-21T09:46:34.622227Z","shell.execute_reply.started":"2021-10-21T09:46:18.365885Z","shell.execute_reply":"2021-10-21T09:46:34.621102Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"df=data.groupby('state_id').mean()\ndf.reset_index(level=0,inplace=True)\nplt.figure(figsize=(20,10))\nplt.subplot(121)\nsns.barplot(x='state_id',y='demand',data=df)\nplt.title(\"Bar-Graph for Avg Sales According to each state\")\n\ndf=data.groupby('state_id').sum()\ndf.reset_index(level=0,inplace=True)\ndf['perc']=df['demand']/sum(df['demand'].values)*100\nplt.subplot(122)\nplt.pie(df['perc'].values,labels=df['state_id'].values,shadow=True,autopct='%1.1f%%')\nplt.title(\"Pie Chart Total showing sales for each state\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:46:34.625995Z","iopub.execute_input":"2021-10-21T09:46:34.626707Z","iopub.status.idle":"2021-10-21T09:46:54.603913Z","shell.execute_reply.started":"2021-10-21T09:46:34.626659Z","shell.execute_reply":"2021-10-21T09:46:54.600747Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"California has the most sales. The reason might be because California has the most population compares to the other two states.","metadata":{}},{"cell_type":"code","source":"df=data.groupby('store_id').mean()\ndf.reset_index(level=0,inplace=True)\nplt.figure(figsize=(20,10))\nplt.subplot(121)\nsns.barplot(x='store_id',y='demand',data=df)\nplt.title(\"Bar-Graph for  AVG Sales According to each store\")\ndf=data.groupby('store_id').sum()\ndf.reset_index(level=0,inplace=True)\ndf['perc']=df['demand']/sum(df['demand'].values)*100\nplt.subplot(122)\nplt.pie(df['perc'].values,labels=df['store_id'].values,shadow=True,autopct='%1.1f%%')\nplt.title(\"Pie Chart showing total sales for each store\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:46:54.605388Z","iopub.execute_input":"2021-10-21T09:46:54.605664Z","iopub.status.idle":"2021-10-21T09:47:15.145201Z","shell.execute_reply.started":"2021-10-21T09:46:54.605629Z","shell.execute_reply":"2021-10-21T09:47:15.144287Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"At store CA_3, total sales is the most and it might be a cause of highest state sales.\nStore WI_2 is also interesting, it has a lot of sales compares to another stores in the state.","metadata":{}},{"cell_type":"code","source":"sns.catplot(x='store_id', y='total_sales',\n           hue='cat_id',\n           data=sales_train_validation, kind='bar',\n           height=8, aspect=1)\nplt.title('Total Sales by Store')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:47:15.146419Z","iopub.execute_input":"2021-10-21T09:47:15.146646Z","iopub.status.idle":"2021-10-21T09:47:19.282045Z","shell.execute_reply.started":"2021-10-21T09:47:15.146616Z","shell.execute_reply":"2021-10-21T09:47:19.281012Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Usually HOUSEHOLD product sales more than HOBBIES, but some store has HOBBIES product sales more than HOUSEHOLD.","metadata":{}},{"cell_type":"code","source":"df=data.groupby('dept_id').mean()\ndf.reset_index(level=0,inplace=True)\nplt.figure(figsize=(20,10))\nplt.subplot(121)\nsns.barplot(x='dept_id',y='demand',data=df)\nplt.title(\"Bar-Graph for  AVG Sales According to each department\")\ndf=data.groupby('dept_id').sum()\ndf.reset_index(level=0,inplace=True)\ndf['perc']=df['demand']/sum(df['demand'].values)*100\nplt.subplot(122)\nplt.pie(df['perc'].values,labels=df['dept_id'].values,shadow=True,autopct='%1.1f%%')\nplt.title(\"Pie Chart showing total sales for each department\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:47:19.283806Z","iopub.execute_input":"2021-10-21T09:47:19.284143Z","iopub.status.idle":"2021-10-21T09:47:40.066714Z","shell.execute_reply.started":"2021-10-21T09:47:19.284096Z","shell.execute_reply":"2021-10-21T09:47:40.065784Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Almost 50% of the sales are done by FOODS_3 department.","metadata":{}},{"cell_type":"code","source":"df=data.groupby('month').mean()\ndf.reset_index(level=0,inplace=True)\nplt.figure(figsize=(20,10))\nplt.subplot(121)\nsns.lineplot(x='month',y='demand',data=df, marker=\"o\")\nplt.yticks(np.arange(0.7, 1.5, 0.1))\nplt.title(\"Line plot for AVG Sales According to each month\")\n\ndf=data.groupby('month').sum()\ndf.reset_index(level=0,inplace=True)\ndf['perc']=df['demand']/sum(df['demand'].values)*100\nplt.subplot(122)\nplt.pie(df['perc'].values,labels=df['month'].values,shadow=True,autopct='%1.1f%%')\nplt.title(\"Pie Chart showing Total sales for each month\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:47:40.068474Z","iopub.execute_input":"2021-10-21T09:47:40.069105Z","iopub.status.idle":"2021-10-21T09:47:55.634869Z","shell.execute_reply.started":"2021-10-21T09:47:40.069059Z","shell.execute_reply":"2021-10-21T09:47:55.634180Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"There are some seasonality in month sales data.","metadata":{}},{"cell_type":"code","source":"hobbies_state = sales_train_validation.loc[(sales_train_validation['cat_id'] == 'HOBBIES')].groupby(['state_id']).mean().T\nhobbies_state = hobbies_state.rename({'CA': 'HOBBIES_CA', 'TX': 'HOBBIES_TX', 'WI': 'HOBBIES_WI'}, axis=1)\nhousehold_state = sales_train_validation.loc[(sales_train_validation['cat_id'] == 'HOUSEHOLD')].groupby(['state_id']).mean().T\nhousehold_state = household_state.rename({'CA': 'HOUSEHOLD_CA', 'TX': 'HOUSEHOLD_TX', 'WI': 'HOUSEHOLD_WI'}, axis=1)\nfoods_state = sales_train_validation.loc[(sales_train_validation['cat_id'] == 'FOODS')].groupby(['state_id']).mean().T\nfoods_state = foods_state.rename({'CA': 'FOODS_CA', 'TX': 'FOODS_TX', 'WI': 'FOODS_WI'}, axis=1)\nnine_example = pd.concat([hobbies_state, household_state, foods_state], axis=1)\nnine_example = nine_example.drop('total_sales')","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:47:55.635885Z","iopub.execute_input":"2021-10-21T09:47:55.636399Z","iopub.status.idle":"2021-10-21T09:47:56.480831Z","shell.execute_reply.started":"2021-10-21T09:47:55.636342Z","shell.execute_reply":"2021-10-21T09:47:56.479800Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n\nfig, axs = plt.subplots(3, 3, figsize=(15,15))\naxs = axs.flatten()\nax_idx = 0\nfor item in nine_example.columns:\n    nine_example[item].plot(title=item, color=next(color_cycle), ax=axs[ax_idx])\n    ax_idx += 1\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:47:56.482291Z","iopub.execute_input":"2021-10-21T09:47:56.482544Z","iopub.status.idle":"2021-10-21T09:47:57.940585Z","shell.execute_reply.started":"2021-10-21T09:47:56.482514Z","shell.execute_reply":"2021-10-21T09:47:57.939705Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"event_date = calendar.loc[calendar['event_name_1'].isin(calendar.event_name_1.unique()[1:])].d\nFOODS_event = sales_train_validation.loc[(sales_train_validation['cat_id'] == 'FOODS')].groupby(['state_id']).mean().T.reset_index()\nFOODS_event = FOODS_event.loc[FOODS_event['index'].isin(event_date)]\nplt.figure(figsize=(15, 10))\nplt.subplot(3,1,1)\nnine_example['FOODS_CA'].plot(title='FOODS_CA', color=next(color_cycle))\nplt.scatter(FOODS_event.reset_index().level_0, FOODS_event['CA'],color=next(color_cycle), zorder=10)\nplt.subplot(3,1,2)\nnine_example['FOODS_TX'].plot(title='FOODS_TX', color=next(color_cycle))\nplt.scatter(FOODS_event.reset_index().level_0, FOODS_event['TX'],color=next(color_cycle), zorder=10)\nplt.subplot(3,1,3)\nnine_example['FOODS_WI'].plot(title='FOODS_WI', color=next(color_cycle))\nplt.scatter(FOODS_event.reset_index().level_0, FOODS_event['WI'],color=next(color_cycle), zorder=10)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:47:57.941816Z","iopub.execute_input":"2021-10-21T09:47:57.942126Z","iopub.status.idle":"2021-10-21T09:47:59.030764Z","shell.execute_reply.started":"2021-10-21T09:47:57.942090Z","shell.execute_reply":"2021-10-21T09:47:59.030029Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"HOBBIES_event = sales_train_validation.loc[(sales_train_validation['cat_id'] == 'HOBBIES')].groupby(['state_id']).mean().T.reset_index()\nHOBBIES_event = HOBBIES_event.loc[HOBBIES_event['index'].isin(event_date)]\nplt.figure(figsize=(15, 10))\nplt.subplot(3,1,1)\nnine_example['HOBBIES_CA'].plot(title='HOBBIES_CA', color=next(color_cycle))\nplt.scatter(HOBBIES_event.reset_index().level_0, HOBBIES_event['CA'],color=next(color_cycle), zorder=10)\nplt.subplot(3,1,2)\nnine_example['HOBBIES_TX'].plot(title='HOBBIES_TX', color=next(color_cycle))\nplt.scatter(HOBBIES_event.reset_index().level_0, HOBBIES_event['TX'],color=next(color_cycle), zorder=10)\nplt.subplot(3,1,3)\nnine_example['HOBBIES_WI'].plot(title='HOBBIES_WI', color=next(color_cycle))\nplt.scatter(HOBBIES_event.reset_index().level_0, HOBBIES_event['WI'],color=next(color_cycle), zorder=10)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:47:59.031953Z","iopub.execute_input":"2021-10-21T09:47:59.032482Z","iopub.status.idle":"2021-10-21T09:47:59.982142Z","shell.execute_reply.started":"2021-10-21T09:47:59.032443Z","shell.execute_reply":"2021-10-21T09:47:59.981160Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"HOUSEHOLD_event = sales_train_validation.loc[(sales_train_validation['cat_id'] == 'HOUSEHOLD')].groupby(['state_id']).mean().T.reset_index()\nHOUSEHOLD_event = HOUSEHOLD_event.loc[HOUSEHOLD_event['index'].isin(event_date)]\nplt.figure(figsize=(15, 10))\nplt.subplot(3,1,1)\nnine_example['HOUSEHOLD_CA'].plot(title='HOUSEHOLD_CA', color=next(color_cycle))\nplt.scatter(HOUSEHOLD_event.reset_index().level_0, HOUSEHOLD_event['CA'],color=next(color_cycle), zorder=10)\nplt.subplot(3,1,2)\nnine_example['HOUSEHOLD_TX'].plot(title='HOUSEHOLD_TX', color=next(color_cycle))\nplt.scatter(HOUSEHOLD_event.reset_index().level_0, HOUSEHOLD_event['TX'],color=next(color_cycle), zorder=10)\nplt.subplot(3,1,3)\nnine_example['HOUSEHOLD_WI'].plot(title='HOUSEHOLD_WI', color=next(color_cycle))\nplt.scatter(HOUSEHOLD_event.reset_index().level_0, HOUSEHOLD_event['WI'],color=next(color_cycle), zorder=10)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:47:59.983489Z","iopub.execute_input":"2021-10-21T09:47:59.983751Z","iopub.status.idle":"2021-10-21T09:48:01.009141Z","shell.execute_reply.started":"2021-10-21T09:47:59.983716Z","shell.execute_reply":"2021-10-21T09:48:01.008071Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"df=data.groupby(['year','month','state_id']).sum()\ndf.reset_index(level=[0,1,2],inplace=True)\nplt.figure(figsize=(20,25))\nplt.subplot(411)\nplt.plot(df[df['state_id']==\"CA\"]['demand'].values,label=\"CA\")\nplt.plot(df[df['state_id']==\"TX\"]['demand'].values,label=\"TX\")\nplt.plot(df[df['state_id']==\"WI\"]['demand'].values,label=\"WI\")\nplt.xlabel('Months in incresing order of years')\nplt.ylabel('sales')\nplt.title(\"Sales of each state in Increasing order of months for each year\")\nplt.legend()\n\nplt.subplot(412)\nsns.distplot(df[df['state_id']==\"CA\"]['demand'].values)\nplt.title(\"Distribution of sales for each month in various years for states of CA\")\n\nplt.subplot(413)\nsns.distplot(df[df['state_id']==\"TX\"]['demand'].values)\nplt.title(\"Distribution of sales for each month in various years for state of TX\")\n\nplt.subplot(414)\nsns.distplot(df[df['state_id']==\"WI\"]['demand'].values)\nplt.title(\"Distribution of sales for each month in various years for state of WI\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:48:01.010995Z","iopub.execute_input":"2021-10-21T09:48:01.011305Z","iopub.status.idle":"2021-10-21T09:48:13.237043Z","shell.execute_reply.started":"2021-10-21T09:48:01.011257Z","shell.execute_reply":"2021-10-21T09:48:13.235980Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"df=data.groupby(['year','month','cat_id']).sum()\ndf.reset_index(level=[0,1,2],inplace=True)\nplt.figure(figsize=(20,25))\nplt.subplot(411)\nplt.plot(df[df['cat_id']==\"HOBBIES\"]['demand'].values,label=\"HOOBIES\")\nplt.plot(df[df['cat_id']==\"FOODS\"]['demand'].values,label=\"FOODS\")\nplt.plot(df[df['cat_id']==\"HOUSEHOLD\"]['demand'].values,label=\"HOUSEHOLD\")\nplt.xlabel('Months in incresing order of years')\nplt.ylabel('sales')\nplt.title(\"Sales of each category in Increasing order of months for each year\")\nplt.legend()\nplt.subplot(412)\nsns.distplot(df[df['cat_id']==\"HOBBIES\"]['demand'].values)\nplt.title(\"Distribution of sales for each month in various years HOBBIES Category\")\n\nplt.subplot(413)\nsns.distplot(df[df['cat_id']==\"FOODS\"]['demand'].values)\nplt.title(\"Distribution of sales for each month in various years FOODS Category\")\n\nplt.subplot(414)\nsns.distplot(df[df['cat_id']==\"HOUSEHOLD\"]['demand'].values)\nplt.title(\"Distribution of sales for each month in various years HOUSEHOLD Category\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:48:13.238939Z","iopub.execute_input":"2021-10-21T09:48:13.239274Z","iopub.status.idle":"2021-10-21T09:48:25.580543Z","shell.execute_reply.started":"2021-10-21T09:48:13.239233Z","shell.execute_reply":"2021-10-21T09:48:25.579562Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Linear Regression","metadata":{}},{"cell_type":"code","source":"# Function to merge datasets returning only one dataframe.\ndef reshape_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows = 30000000, merge = False):\n    \n    # Reshaping sales data using melt.\n    sales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    print ('Melted sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n    sales_trian_validation = reduce_mem_usage(sales_train_validation)\n    \n    # Preparing test and validation records.\n    test_rows = [row for row in submission['id'] if 'validation' in row]\n    val_rows = [row for row in submission['id'] if 'evaluation' in row]\n    \n    test = submission[submission ['id']. isin(test_rows)]\n    val = submission[submission ['id']. isin(val_rows)]\n    \n    # Renaming the columns.\n    test.columns = ['id', 'd_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919', 'd_1920', 'd_1921',\n                    'd_1922', 'd_1923', 'd_1924', 'd_1925', 'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930',\n                    'd_1931', 'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939',\n                    'd_1940', 'd_1941']\n    val.columns = ['id', 'd_1942', 'd_1943', 'd_1944', 'd_1945', 'd_1946', 'd_1947', 'd_1948', 'd_1949',\n                   'd_1950', 'd_1951', 'd_1952', 'd_1953', 'd_1954', 'd_1955', 'd_1956', 'd_1957', 'd_1958',\n                   'd_1959', 'd_1960', 'd_1961', 'd_1962', 'd_1963', 'd_1964', 'd_1965', 'd_1966', 'd_1967',\n                   'd_1968', 'd_1969']\n    \n    # Getting only product data and removing duplicate records.\n    product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']]. drop_duplicates ()\n    \n    # Merge with the product table.\n    test = test.merge(product, how = 'left', on = 'id')\n    val = val.merge(product, how = 'left', on = 'id')\n    \n    # Reshaping test and validation data.\n    test = pd.melt(test, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    val = pd.melt(val, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    \n    # Creating a new column to define training, test and validation data.\n    sales_train_validation['part'] = 'train'\n    test['part'] = 'test'\n    val['part'] = 'val'\n    \n    # Creating a single dataset with the addition of all training, validation and test records.\n    data = pd.concat([sales_train_validation, test, val], axis = 0)\n    \n    # Removing previous datasets.\n    del sales_train_validation, test, val\n    \n    # Selecting only a few records for training.\n    data = data.loc[nrows:]\n    \n    # Removing validation data.\n    data = data[data ['part']!= 'val']\n    \n    # Performing the merge with calendar and price.\n    if merge:\n        data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n        data.drop(['d', 'day', 'weekday'], inplace = True, axis = 1)\n        data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n        print('Final dataset for training has {} rows and {} columns'.format(data.shape [0], data.shape [1]))\n        del calendar,sell_prices,product\n        gc.collect()\n    else:\n        pass\n    \n    return data\n\n# Function for handling missing values and transformation of categorical/numeric features\ndef transform2(data):\n    \n    # Performing treatment on missing values for the categorical features.\n    nan_features_cat = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in nan_features_cat:\n        data[feature] .fillna('unknown', inplace = True)\n    \n    # Performing treatment on missing values for the sell_price feature.\n    data['sell_price']. fillna(0, inplace = True)\n        \n    # Turning features categories into numbers to make predictions.\n    encoder = preprocessing.LabelEncoder ()\n    data['id_encode'] = encoder.fit_transform(data ['id'])\n    \n    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n           'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in cat:\n        encoder = preprocessing.LabelEncoder()\n        data[feature] = encoder.fit_transform(data[feature])\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:06:14.020247Z","iopub.execute_input":"2021-10-21T05:06:14.020612Z","iopub.status.idle":"2021-10-21T05:06:14.044344Z","shell.execute_reply.started":"2021-10-21T05:06:14.020574Z","shell.execute_reply":"2021-10-21T05:06:14.043363Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def plotModelResults(model, X_train, X_test, plot_intervals=False, plot_anomalies=False):\n    \"\"\"\n        Plots modeled vs fact values, prediction intervals and anomalies\n    \n    \"\"\"\n    \n    prediction = model.predict(X_test)\n    \n    plt.figure(figsize=(15, 7))\n    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n    \n    if plot_intervals:\n        cv = cross_val_score(model, X_train, y_train, \n                                    cv=tscv, \n                                    scoring=\"neg_mean_absolute_error\")\n        mae = cv.mean() * (-1)\n        deviation = cv.std()\n        \n        scale = 1.96\n        lower = prediction - (mae + scale * deviation)\n        upper = prediction + (mae + scale * deviation)\n        \n        plt.plot(lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n        plt.plot(upper, \"r--\", alpha=0.5)\n        \n        if plot_anomalies:\n            anomalies = np.array([np.NaN]*len(y_test))\n            anomalies[y_test<lower] = y_test[y_test<lower]\n            anomalies[y_test>upper] = y_test[y_test>upper]\n            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n    \n    error = rmse(y_test, prediction)\n    plt.title(\"RMSE: {0:.2f}\".format(error))\n    plt.legend(loc=\"best\")\n    plt.tight_layout()\n    plt.grid(True);\n    \ndef plotCoefficients(model):\n    \"\"\"\n        Plots sorted coefficient values of the model\n    \"\"\"\n    \n    coefs = pd.DataFrame(model.coef_, X_train.columns)\n    coefs.columns = [\"coef\"]\n    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n    \n    plt.figure(figsize=(15, 7))\n    coefs.coef.plot(kind='bar')\n    plt.grid(True, axis='y')\n    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:04:51.118875Z","iopub.execute_input":"2021-10-21T05:04:51.119808Z","iopub.status.idle":"2021-10-21T05:04:51.135666Z","shell.execute_reply.started":"2021-10-21T05:04:51.119750Z","shell.execute_reply":"2021-10-21T05:04:51.134772Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def prepare_data_lr(data, lag_start, lag_end, test_size, target_encoding=False):\n\n    # lags of series\n    for i in range(7, 29):\n        data[\"lag_{}\".format(i)] = data['demand'].shift(i)\n\n    # datetime features\n    data.index = pd.to_datetime(data.index)\n    data[\"day\"] = data.index.day\n    data[\"weekday\"] = data.index.weekday\n    data['is_weekend'] = data.weekday.isin([5,6])*1\n\n    if target_encoding:\n        # calculate averages on train set only\n        test_index = int(len(data.dropna())*(1-test_size))\n        data['weekday_average'] = list(map(code_mean(data[:test_index], 'weekday', \"demand\").get, data['weekday']))\n        data[\"day_average\"] = list(map(code_mean(data[:test_index], 'day', \"demand\").get, data['day']))\n\n        # frop encoded variables \n        data.drop([\"day\", \"weekday\"], axis=1, inplace=True)\n    \n    # train-test split\n    y = data.dropna()['demand']\n    X = data.dropna().drop(['demand'], axis=1)\n\n    X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=test_size)\n\n    return X_train, X_test, y_train, y_test","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:04:52.814905Z","iopub.execute_input":"2021-10-21T05:04:52.815392Z","iopub.status.idle":"2021-10-21T05:04:52.827602Z","shell.execute_reply.started":"2021-10-21T05:04:52.815346Z","shell.execute_reply":"2021-10-21T05:04:52.826473Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def code_mean(data, cat_feature, real_feature):\n    \"\"\"\n    Returns a dictionary where keys are unique categories of the cat_feature,\n    and values are means over real_feature\n    \"\"\"\n    return dict(data.groupby(cat_feature)[real_feature].mean())\n\ndef timeseries_train_test_split(X, y, test_size):\n    \"\"\"\n        Perform train-test split with respect to time series structure\n    \"\"\"\n    \n    # get the index after which test set starts\n    test_index = int(len(X)*(1-test_size))\n    \n    X_train = X.iloc[:test_index]\n    y_train = y.iloc[:test_index]\n    X_test  = X.iloc[test_index:]\n    y_test  = y.iloc[test_index:]\n    \n    return X_train, X_test, y_train, y_test\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:04:54.172594Z","iopub.execute_input":"2021-10-21T05:04:54.173071Z","iopub.status.idle":"2021-10-21T05:04:54.181467Z","shell.execute_reply.started":"2021-10-21T05:04:54.173015Z","shell.execute_reply":"2021-10-21T05:04:54.180747Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"calendar, sell_prices, sales_train_validation, submission = read_data(False)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:04:56.367767Z","iopub.execute_input":"2021-10-21T05:04:56.368081Z","iopub.status.idle":"2021-10-21T05:05:08.153598Z","shell.execute_reply.started":"2021-10-21T05:04:56.368049Z","shell.execute_reply":"2021-10-21T05:05:08.151988Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Reshaping and merging datasets.\ndata = reshape_and_merge(calendar, sell_prices, sales_train_validation, submission, merge = True)\n\n# Calling up the data transformation functions.\ndata = transform2(data)\n\n# Viewing the final dataset header.\ndata.head()\n\n# Clearing data from memory.\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:06:19.589145Z","iopub.execute_input":"2021-10-21T05:06:19.589437Z","iopub.status.idle":"2021-10-21T05:11:00.909100Z","shell.execute_reply.started":"2021-10-21T05:06:19.589407Z","shell.execute_reply":"2021-10-21T05:11:00.908407Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Selecting only 1 item for testing: FOODS_3_634_WI_2.\ndf = data [(data ['date'] <= '2016-04-24') & (data ['id'] == 'FOODS_3_634_WI_2_validation') & (data ['demand'] > 0) & (data['demand'] <= 15)]\n\n# Selecting only a few columns for analysis and training.\ndf = df[['date', 'demand', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'snap_WI', 'sell_price']]\n\n# Transforming the date as index.\ndf = df.set_index('date')","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:11:30.649582Z","iopub.execute_input":"2021-10-21T05:11:30.649955Z","iopub.status.idle":"2021-10-21T05:11:43.900226Z","shell.execute_reply.started":"2021-10-21T05:11:30.649916Z","shell.execute_reply":"2021-10-21T05:11:43.899313Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"tscv = TimeSeriesSplit (n_splits = 5)\n\nX_train, X_test, y_train, y_test = prepare_data_lr(df, lag_start=1, \n                                               lag_end=29, test_size=0.1, \n                                               target_encoding=True)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)\n\nplotModelResults(lr, X_train=X_train_scaled, X_test=X_test_scaled, plot_intervals=True, plot_anomalies=True)\nplotCoefficients(lr)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:12:05.672694Z","iopub.execute_input":"2021-10-21T05:12:05.673232Z","iopub.status.idle":"2021-10-21T05:12:06.795940Z","shell.execute_reply.started":"2021-10-21T05:12:05.673170Z","shell.execute_reply":"2021-10-21T05:12:06.795331Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"ridge = RidgeCV(cv=tscv)\nridge.fit(X_train_scaled, y_train)\n\nplotModelResults(ridge, \n                 X_train=X_train_scaled, \n                 X_test=X_test_scaled, \n                 plot_intervals=True, \n                 plot_anomalies=True)\nplotCoefficients(ridge)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T05:13:19.760806Z","iopub.execute_input":"2021-10-21T05:13:19.761138Z","iopub.status.idle":"2021-10-21T05:13:21.000855Z","shell.execute_reply.started":"2021-10-21T05:13:19.761104Z","shell.execute_reply":"2021-10-21T05:13:20.999885Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## LGBM Regressor","metadata":{}},{"cell_type":"code","source":"calendar, sell_prices, sales_train_validation, submission = read_data(True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = melt_and_merge(calendar, sell_prices, sales_train_validation,\n                      submission, nrows = 30000000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = transform(data)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T13:45:14.819322Z","iopub.execute_input":"2021-10-20T13:45:14.819624Z","iopub.status.idle":"2021-10-20T13:47:07.036005Z","shell.execute_reply.started":"2021-10-20T13:45:14.819584Z","shell.execute_reply":"2021-10-20T13:47:07.035033Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"data = features_engineering(data)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T13:47:07.037654Z","iopub.execute_input":"2021-10-20T13:47:07.037992Z","iopub.status.idle":"2021-10-20T13:56:46.733254Z","shell.execute_reply.started":"2021-10-20T13:47:07.037949Z","shell.execute_reply":"2021-10-20T13:56:46.731964Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"x_train, y_train, x_val, y_val, test = split_data(data)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T13:56:46.734901Z","iopub.execute_input":"2021-10-20T13:56:46.735217Z","iopub.status.idle":"2021-10-20T13:57:05.747470Z","shell.execute_reply.started":"2021-10-20T13:56:46.735177Z","shell.execute_reply":"2021-10-20T13:57:05.746357Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"features = ['item_id', 'cat_id', 'state_id', 'year', 'month', 'week', 'day', 'dayofweek', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n            'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_t28', 'lag_t29', 'lag_t30', 'rolling_mean_t7', 'rolling_std_t7', 'rolling_mean_t30', 'rolling_mean_t90', \n            'rolling_mean_t180', 'rolling_std_t30', 'price_change_t1', 'price_change_t365', 'rolling_price_std_t7', 'rolling_price_std_t30', 'rolling_skew_t30', 'rolling_kurt_t30',\n            'isweekend','lag_revenue_t1','rolling_revenue_std_t28','rolling_revenue_mean_t28']\ncategory = x_train['dept_id'].unique()\ncategory2 = x_train['store_id'].unique()\n\n\nfrom lightgbm import Dataset,train,plot_importance\nparams = {\n    'boosting_type': 'gbdt',\n    'metric': 'rmse',\n    'objective': 'regression',\n    'n_jobs': -1,\n    'seed': 236,\n    'learning_rate': 0.1}\n\nTEST = None\n#i=0\n#feature_importances = pd.DataFrame()\n#feature_importances['feature'] = features\n\nfor i in category:\n    for j in category2:\n        x_t,x_v,y_t,y_v,t = dataset(i,j)\n        train_set = Dataset(x_t[features], y_t)\n        val_set = Dataset(x_v[features], y_v)\n        del x_t, y_t\n        gc.collect()\n\n        model = train(params, train_set, num_boost_round = 5000, early_stopping_rounds = 40, valid_sets = [train_set, val_set], verbose_eval = 1000)\n        y_pred = model.predict(t[features])\n        t['demand'] = y_pred\n        #feature_importances[i] = model.feature_importance()\n        #i+=1\n        \n        TEST = pd.concat([TEST,t],axis=0)\n        del x_v, y_v, t, train_set, val_set, y_pred, model\n        gc.collect()\n\n'''\nfeature_importances['average'] = feature_importances[[i for i in range()]].mean(axis=1)\nfeature_importances.to_csv('lgb_feature_importances.csv')\n\nplt.figure(figsize=(16, 12))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(20), x='average', y='feature');\nplt.title('20 TOP feature importance over {} folds average'.format(folds.n_splits));\n'''","metadata":{"execution":{"iopub.status.busy":"2021-10-20T13:57:05.748903Z","iopub.execute_input":"2021-10-20T13:57:05.749309Z","iopub.status.idle":"2021-10-20T15:37:23.068798Z","shell.execute_reply.started":"2021-10-20T13:57:05.749264Z","shell.execute_reply":"2021-10-20T15:37:23.068202Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"predictions1 = TEST[TEST['id'].apply(lambda x: \"validation\" in x)][['id', 'date', 'demand']]\npredictions2 = TEST[TEST['id'].apply(lambda x: \"evaluation\" in x)][['id', 'date', 'demand']]","metadata":{"execution":{"iopub.status.busy":"2021-10-20T15:37:23.069854Z","iopub.execute_input":"2021-10-20T15:37:23.070463Z","iopub.status.idle":"2021-10-20T15:37:24.401207Z","shell.execute_reply.started":"2021-10-20T15:37:23.070431Z","shell.execute_reply":"2021-10-20T15:37:24.400488Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"prediction1 = pd.pivot(predictions1, index = 'id', columns = 'date', values = 'demand').reset_index()\nprediction1.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]","metadata":{"execution":{"iopub.status.busy":"2021-10-20T15:37:24.402727Z","iopub.execute_input":"2021-10-20T15:37:24.403197Z","iopub.status.idle":"2021-10-20T15:37:24.861673Z","shell.execute_reply.started":"2021-10-20T15:37:24.403163Z","shell.execute_reply":"2021-10-20T15:37:24.861011Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"prediction2 = pd.pivot(predictions2, index = 'id', columns = 'date', values = 'demand').reset_index()\nprediction2.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]","metadata":{"execution":{"iopub.status.busy":"2021-10-20T15:37:24.862703Z","iopub.execute_input":"2021-10-20T15:37:24.863332Z","iopub.status.idle":"2021-10-20T15:37:25.293006Z","shell.execute_reply.started":"2021-10-20T15:37:24.863295Z","shell.execute_reply":"2021-10-20T15:37:25.292063Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"predictions = pd.concat([prediction1,prediction2],axis=0)\npredictions.to_csv(\"lgbm_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T15:37:25.294282Z","iopub.execute_input":"2021-10-20T15:37:25.294630Z","iopub.status.idle":"2021-10-20T15:37:29.220273Z","shell.execute_reply.started":"2021-10-20T15:37:25.294596Z","shell.execute_reply":"2021-10-20T15:37:29.219351Z"},"trusted":true},"execution_count":20,"outputs":[]}]}